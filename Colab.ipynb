{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN75fBXXoFGvkkv1JHXOcCk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thisisamirv/ASD_rs-fmri_CapsNet/blob/main/Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing packages\n",
        "!pip install torch torchvision numpy\n",
        "!pip install git+https://github.com/arogozhnikov/einops.git#egg=einops"
      ],
      "metadata": {
        "id": "_xQ6XPqig7q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops.layers.torch import EinMix as Mix, Rearrange\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "from einops import rearrange, reduce, asnumpy\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Oc-2N4TQl9Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CapsNet\n",
        "def squash(x, dim):\n",
        "    \"\"\" Non-linear activation, that squashes all vectors to have norm < 1 \"\"\"\n",
        "    norm_sq = torch.sum(x ** 2, dim, keepdim=True)\n",
        "    norm = torch.sqrt(norm_sq)\n",
        "    return (norm_sq / (1.0 + norm_sq)) * (x / norm)\n",
        "\n",
        "\n",
        "class CapsuleLayerWithRouting(nn.Module):\n",
        "    def __init__(self, in_caps, in_hid, out_caps, out_hid):\n",
        "        super().__init__()\n",
        "        self.input_caps2U = Mix(\n",
        "            'b in_caps in_hid -> b in_caps out_caps out_hid',\n",
        "            weight_shape='in_caps in_hid out_caps out_hid',\n",
        "            in_hid=in_hid, in_caps=in_caps, out_hid=out_hid, out_caps=out_caps,\n",
        "        )\n",
        "\n",
        "    def forward(self, input_capsules, routing_iterations):\n",
        "        U = self.input_caps2U(input_capsules)\n",
        "        batch, in_caps, out_caps, out_hid = U.shape\n",
        "\n",
        "        # logsoftmax for connections between capsules\n",
        "        B = torch.zeros([batch, in_caps, out_caps], device=U.device)\n",
        "\n",
        "        # routing algorithm (procedure 1 from paper)\n",
        "        # names of axes: b=batch, i=input capsules, o=output_capsules, h=hidden dim of output capsule\n",
        "        for _ in range(routing_iterations):\n",
        "            # \"routing softmax\" determines connection between capsules in layers\n",
        "            C = torch.softmax(B, dim=-1)\n",
        "            S = torch.einsum('bio,bioh->boh', C, U)\n",
        "            V = squash(S, dim=-1)\n",
        "            B = B + torch.einsum('bioh,boh->bio', U, V)\n",
        "        return V\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_h, in_w, in_c,\n",
        "                 n_primary_caps_groups, primary_caps_dim,\n",
        "                 n_digit_caps, digit_caps_dim,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.image2primary_capsules = nn.Sequential(\n",
        "            nn.Conv2d(in_c, 256, kernel_size=9),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, n_primary_caps_groups * primary_caps_dim, kernel_size=9, stride=2),\n",
        "            # regroup conv output into flat capsules\n",
        "            Rearrange('b (caps hid) h w -> b (h w caps) hid', caps=n_primary_caps_groups, hid=primary_caps_dim),\n",
        "        )\n",
        "        # figure out correct number of capsules by passing a test image through, lazy but simple\n",
        "        _, n_primary_capsules, _ = self.image2primary_capsules(torch.zeros(1, in_c, in_h, in_w)).shape\n",
        "        self.primary2digit_capsules = CapsuleLayerWithRouting(\n",
        "            in_caps=n_primary_capsules, in_hid=primary_caps_dim,\n",
        "            out_caps=n_digit_caps, out_hid=digit_caps_dim,\n",
        "        )\n",
        "\n",
        "    def forward(self, images, routing_iterations=3):\n",
        "        primary_capsules = self.image2primary_capsules(images) * 0.01  # scaling 0.01 to get norms not too close to 1\n",
        "        return self.primary2digit_capsules(primary_capsules, routing_iterations)\n",
        "\n",
        "\n",
        "def Decoder(n_caps, caps_dim, output_h, output_w, output_channels):\n",
        "    return nn.Sequential(\n",
        "        Mix('b caps caps_dim -> b hidden', weight_shape='caps caps_dim hidden', caps=n_caps, caps_dim=caps_dim, hidden=512),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(512, 1024),\n",
        "        nn.ReLU(inplace=True),\n",
        "        Mix('b hidden -> b c h w', weight_shape='hidden c h w', hidden=1024, h=output_h, w=output_w, c=output_channels),\n",
        "        nn.Sigmoid(),\n",
        "    )\n"
      ],
      "metadata": {
        "id": "T4yEEgJtmIOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "def load_mnist(batch_size, workers=0):\n",
        "    train_transform = transforms.Compose([\n",
        "        # small random shifts\n",
        "        transforms.RandomAffine(degrees=(0, 0), translate=(0.1, 0.1), fill=0),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    test_transform = transforms.ToTensor()\n",
        "\n",
        "    training_data_loader = DataLoader(\n",
        "        datasets.MNIST('./data', train=True, download=True, transform=train_transform),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "\n",
        "    testing_data_loader = DataLoader(\n",
        "        datasets.MNIST('./data', train=False, download=True, transform=test_transform),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "\n",
        "    image_shape = (28, 28, 1)\n",
        "    n_classes = 10\n",
        "    return training_data_loader, testing_data_loader, image_shape, n_classes\n",
        "\n",
        "\n",
        "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_loader, test_loader, (image_h, image_w, image_c), n_classes = load_mnist(batch_size=64, workers=0)\n",
        "\n",
        "encoder = Encoder(\n",
        "    in_h=image_h, in_w=image_w, in_c=image_c,\n",
        "    n_primary_caps_groups=32, primary_caps_dim=8,\n",
        "    n_digit_caps=n_classes, digit_caps_dim=16\n",
        ").to(device)\n",
        "\n",
        "decoder = Decoder(\n",
        "    n_caps=n_classes, caps_dim=16,\n",
        "    output_h=image_h, output_w=image_w, output_channels=image_c,\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam([*encoder.parameters(), *decoder.parameters()])\n",
        "\n",
        "\n",
        "def margin_loss(class_capsules, target_one_hot, m_minus=0.1, m_plus=0.9, loss_lambda=0.5):\n",
        "    caps_norms = torch.norm(class_capsules, dim=2)\n",
        "    assert caps_norms.max() <= 1.001, 'capsules outputs should be bound by unit norm'\n",
        "    # correct capsule is enforced is not penalized if norm > m_plus,\n",
        "    # while incorrect ones are not penalized if norm < m_minus\n",
        "    loss_sig = torch.clamp(m_plus - caps_norms, 0) ** 2\n",
        "    loss_bkg = torch.clamp(caps_norms - m_minus, 0) ** 2\n",
        "\n",
        "    loss = target_one_hot * loss_sig + loss_lambda * (1.0 - target_one_hot) * loss_bkg\n",
        "    return reduce(loss, 'b cls -> b', 'sum')\n",
        "\n",
        "\n",
        "for epoch in range(100):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        digit_capsules = encoder(images.to(device))\n",
        "        labels_one_hot = torch.nn.functional.one_hot(labels, num_classes=10).float().to(device)\n",
        "        loss = margin_loss(digit_capsules, labels_one_hot).mean()\n",
        "        reconstructed = decoder(digit_capsules * rearrange(labels_one_hot, 'b caps -> b caps 1'))\n",
        "        reconstruction_loss_mse = (images.to(device) - reconstructed).pow(2).mean()\n",
        "        loss += reconstruction_loss_mse * 10.  # pick a weight\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    accuracies = []\n",
        "    for images, labels in test_loader:\n",
        "        digit_capsules = encoder(images.to(device)).cpu()\n",
        "        # predicted capsule is capsule with largest norm\n",
        "        predicted_labels = digit_capsules.norm(dim=2).argmax(dim=1)\n",
        "        accuracies += asnumpy(predicted_labels == labels).tolist()\n",
        "\n",
        "    print(f'epoch {epoch} accuracy: {np.mean(accuracies)}')\n"
      ],
      "metadata": {
        "id": "3GoNHemcmP6M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}